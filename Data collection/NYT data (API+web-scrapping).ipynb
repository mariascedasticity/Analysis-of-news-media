{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Libraries #\n",
    "#############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "#Time vars manipulations\n",
    "import time\n",
    "import datetime\n",
    "from dateutil.rrule import rrule, MONTHLY\n",
    "\n",
    "#for JSON requests and manipulations\n",
    "import requests\n",
    "import pyjq\n",
    "from itertools import chain\n",
    "import json\n",
    "\n",
    "#web scrapper\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NYTimes developer key\n",
    "key = YOUR_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sends query\n",
    "url = 'https://api.nytimes.com/svc/archive/v1/2006/10.json?api-key='+key\n",
    "req = requests.get(url)\n",
    "json_data = req.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saves JSON file\n",
    "with open(\"json_data.json\", \"w\") as write_file:\n",
    "    json.dump(json_data, write_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of articles per query (month)\n",
    "num_docs = pyjq.all('.response .docs | length', json_data)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts particular information (variables)\n",
    "jq_query = f'.response .docs [] | {{n_url: .web_url, snippet: .snippet, paragraph: .lead_paragraph, mult: .multimedia[] | .url, headline: .headline .main, keyword: .keywords, date: .pub_date, doc_type: .document_type, news_desk: .news_desk, section: .section_name, subsectoin: .subsectoinName, author: .byline .original, id: ._id, word_count: .word_count}}'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns dict with data\n",
    "output = pyjq.all(jq_query, json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to dataframe\n",
    "df = pd.DataFrame(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop for automated process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create list of (year, month) pairs\n",
    "#(year,month,day)\n",
    "\n",
    "start_dt = datetime.date(2006,1,1)\n",
    "end_dt = datetime.date(2020,7,1)\n",
    "\n",
    "dates = [(dt.year, dt.month) for dt in rrule(MONTHLY, dtstart=start_dt, until=end_dt)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creates an empty datasets with certain columns\n",
    "df = pd.DataFrame(columns = ['n_url', 'snippet', 'lead_paragraph', 'image', 'headline', 'date',\n",
    "       'doc_type', 'news_desk', 'section', 'author', 'id', 'word_count'])\n",
    "\n",
    "#loop for extracting data for month/year pairs\n",
    "for year, month in tqdm(dates):\n",
    "    time.sleep(20) #to prevent attacks\n",
    "    print(year, month)\n",
    "    url = f'https://api.nytimes.com/svc/archive/v1/{year}/{month}.json?api-key={key}'\n",
    "    r = requests.get(url)\n",
    "    js_data = r.json()\n",
    "    \n",
    "    num_docs = pyjq.all('.response .docs | length', js_data)[0]\n",
    "    print(f'For month {month} in {year} there were {num_docs} articles')\n",
    "    \n",
    "    #extract required variables\n",
    "    jq_q = f'.response .docs [] | {{n_url: .web_url, snippet: .snippet, lead_paragraph: .lead_paragraph, image: .multimedia[1].url, headline: .headline .main, date: .pub_date, doc_type: .document_type, news_desk: .news_desk, section: .section_name, author: .byline, id: ._id, word_count: .word_count}}'\n",
    "    out = pyjq.all(jq_q, js_data)\n",
    "    \n",
    "    #to dataframe + appending\n",
    "    g = pd.DataFrame(out)\n",
    "    df = df.append(g,  sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#makes image urls clickable\n",
    "#and attributes none (NaN) for articles without pictures\n",
    "df['image'] = 'https://static01.nyt.com/' + df['image'].astype(str) + '?quality=90&auto=webp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NYT_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Web scrapping (full article texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('NYT_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text(df):\n",
    "    \n",
    "    df['full_text'] = 'NaN'\n",
    "    session = requests.Session()\n",
    "    \n",
    "    for j in tqdm(range(0, len(df))):\n",
    "        print(j)\n",
    "        try:\n",
    "            url = df['n_url'][j]\n",
    "            req = session.get(url)\n",
    "            soup = BeautifulSoup(req.text, 'lxml')\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "        \n",
    "        #Extracting all HTML text under tag 'p'\n",
    "        tags = soup.find_all('p')\n",
    "        if tags == []:\n",
    "            tags = soup.find_all('p', itemprop = 'articleBody')\n",
    "\n",
    "        # Joining HTML text\n",
    "        article = ''\n",
    "        for p in tags:\n",
    "            article = article + ' ' + p.get_text()\n",
    "            article = \" \".join(article.split())\n",
    "\n",
    "        # Text to the DataFrame\n",
    "        df['full_text'][j] = article\n",
    "\n",
    "    return df                          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_full_text(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('NYT_data_text.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Libraries #\n",
    "#############\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import warnings\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.feature_extraction import text \n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_text(df, text_field):\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"http\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\\S+\", \"\")\n",
    "    df[text_field] = df[text_field].str.replace(r\"[^A-Za-z0-9(),!?@\\'\\`\\\"\\_\\n]\", \" \")\n",
    "    df[text_field] = df[text_field].str.replace(r\"@\", \"at\")\n",
    "    df[text_field] = df[text_field].str.lower()\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = standardize_text(df, \"full_text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tokenizing sentences to a list of separate words\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "df[\"tokens\"] = df[\"full_text\"].apply(tokenizer.tokenize)\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")\n",
    "stopword = nltk.corpus.stopwords.words('english')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    text = [word for word in text if word not in stopword]\n",
    "    return text\n",
    "    \n",
    "df['tokens'] = df['tokens'].apply(lambda x: remove_stopwords(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [word for tokens in df[\"tokens\"] for word in tokens]\n",
    "vocabulary = sorted(list(set(words)))\n",
    "\n",
    "print(\"%s words total, with a vocabulary size of %s\" % (len(words), len(vocabulary)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "wnl = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "def lemmatize(s):\n",
    "    s = [wnl.lemmatize(word) for word in s]\n",
    "    return s\n",
    "\n",
    "df = df.assign(lemm = df.tokens.apply(lambda x: lemmatize(x)))\n",
    "\n",
    "df = df.drop(['tokens'], axis=1)\n",
    "df = df.rename(columns={\"lemm\": \"tokens\"})\n",
    "\n",
    "df.to_csv('data.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Lasso Regression + Marginal Effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_df(path):\n",
    "    df = pd.read_csv(path)[[\"id\", \"female\", \"male\", \"gen_label\", \"tokens\"]]\n",
    "    \n",
    "    df_target = df[(df[\"gen_label\"] == 1) | (df[\"gen_label\"] == 2)]\n",
    "    df_notarget = df[(df[\"gen_label\"] == 3) | (df[\"gen_label\"] == 4)]\n",
    "    \n",
    "    return df_target, df_notarget\n",
    "\n",
    "def plot_roc_curve(y_true, y_pred):\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_pred)\n",
    "    plt.plot(fpr, tpr, color='orange', label='ROC')\n",
    "    plt.plot([0, 1], [0, 1], color='darkblue', linestyle='--')\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def save_obj(obj, file_name):\n",
    "    with open(file_name, \"wb\") as file:\n",
    "        pickle.dump(obj, file)\n",
    "        \n",
    "def load_obj(file_name):\n",
    "    with open(file_name, \"rb\") as file:\n",
    "        file = pickle.load(file)\n",
    "    return file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_target, df_notarget = build_df(\"data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_model = df_target[[\"tokens\", \"gen_label\"]]\n",
    "df_model.loc[df_model[\"gen_label\"] == 2, \"gen_label\"]  = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corpus = df_model[\"tokens\"]\n",
    "df_labels = df_model[\"gen_label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vectorizer = CountVectorizer(\n",
    "    max_features=10000, \n",
    "    stop_words= 'english',\n",
    "    min_df = 0.05,\n",
    "    max_df = 0.80\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_corpus, df_labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words = count_vectorizer.fit_transform(X_train)\n",
    "X_test_words = count_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = LogisticRegression(C=1, penalty='l1', \n",
    "                           solver='liblinear', n_jobs=-1, \n",
    "                           random_state=42, class_weight=\"balanced\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(logit, X_train_words, y_train, scoring=\"roc_auc\", cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_roc_curve(y_test, logit.fit(X_train_words, y_train).predict_proba(X_test_words)[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Top 200 words by magnitude of the coefficient\n",
    "most_important_idx = np.argsort(np.abs(logit.coef_[0]))[-200:]\n",
    "\n",
    "most_important_w = logit.coef_[0][most_important_idx]\n",
    "most_important_word = np.array(count_vectorizer.get_feature_names())[most_important_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict(zip(most_important_word, most_important_w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_words = X_train_words.toarray()\n",
    "pred_w = logit.predict_proba(X_train_words)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculating marginal effects of words\n",
    "# ME shows a one unit change in a particular word has on the predicted probability of Y,\n",
    "# holding other words from the bag-of-words fixed \n",
    "\n",
    "marginal_effects = {}\n",
    "\n",
    "for word_idx in tqdm(most_important_idx):\n",
    "    X_train_words_i = X_train_words.copy()\n",
    "    # a one unit increase\n",
    "    X_train_words_i[word_idx] = X_train_words_i[word_idx] + 1  \n",
    "    # Predictions of P(y | W_i + 1)\n",
    "    pred_w_i = logit.predict_proba(X_train_words_i)[:, 1] # Pr(female=1)\n",
    "    \n",
    "    # marginal effect\n",
    "    # P(y | W_i + 1) * (1 - P(y | W_i)) * beta_i\n",
    "    me = (logit.coef_[0][word_idx] / pred_w.shape[0]) * np.sum(pred_w_i * (1 - pred_w))\n",
    "    \n",
    "    word = count_vectorizer.get_feature_names()[word_idx]\n",
    "    marginal_effects[word] = me.round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(marginal_effects.items(), key=lambda t: t[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
